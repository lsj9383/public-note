一、概述
	0.工作分配原理
		依赖jar包，可以将本地写的类在远程执行。
		客户端集成某个固定的抽象类A_CLASS, 将工程生成jar包，远程主机可以通过网络下载该jar包。
		这样，远程主机拥有该jar包后，就可以利用相关方法，得到需要的类，再实例化对象，执行该对象的方法。
	1.分工：
		map 	局部处理
		reduce	汇总工作
	2.输入
		map和reduce的数据的输入输出都是以key-value对的形式封装的。
		4个泛型中，前两个是指定mapper输入数据的类型，即key-value类型。
		1).针对mapper
			KEYIN是输入的key类型，VALUEIN是输入的value类型。
			默认情况下，框架传递给我们的mapper的输入数据中，
			key : 要处理的文本中一行的起始偏移量。
			value : 这一行的类容。
			map将处理结果向reducer传输，是通过其中的context参数实现的。
		2).针对reducer
			KEYIN是mapper中的KEYOUT, VALUEIN是mapper中的Iterator<VALUEOUT>。
			mapper在每次主机读取一行数据的时候执行，并将执行后的结果发送给reducer，但是reducer并不马上执行。、
			reducer是在接收完所有mapper输出的数据后，才执行的。因此reducer将缓存大量的key-value.
			reducer在接收完key-value后，会对key-value进行预处理:
				预处理筛选出所有相同的key，将其value保存在一个集合中，用迭代器进行表示。
				会将数据按key排序(字典顺序)。+
				最后reducer的输入将会是key-values.
		3).序列化
			这些key和value都是对象，要走网络，要序列化。
			jdk自带的序列化存在冗余，hadoop实现了自己的序列化，用hadoop自带的类型，可以提高序列化效率。
			Long<----->LongWritable
			String<--->Text
		4).job
			对一个特定的业务逻辑整体，称为一个job。
			一个job要描述：
				mapper和reducer的分配。
				job要处理数据的路径。
				job结果输出的路径。
			
	3.执行
		mapreduce框架中的datanode主机，每次读一行数据，都将这一行的信息传递给map方法。
		
二、demo
	1.代码
		public class WCMapper extends MApper<Long, String, String, Integer>{
			@Override
			protected void map(LongWritable key, Text value, Context context) 
					throws IOException, InterruptedException{
				//具体业务逻辑，而且业务要处理的数据已经被框架传递尽量，在方法的参数key-value中
				//key是这一行数据的起始偏移量 value是这一行的文本内容
				//context是一个向reducer传输的工具
				String line = value.toString();
				String[] words = StringUtils.split(line, ' ');
				for(String word : words){
					context.write(new Text(word), new LongWritable(1));
				}
			}
		}

		public class WCReducer extends Reducer<Text, LongWritable, Text, LongWritable>{
			//框架在map处理完成后，将所有缓存起来的key-value，进行分组，然后传递一个组，调用一个reduce方法
			//相同的key组成一个组。
			
			@Override
			protected void reduce(Text key, Iterable<LongWritable> values, Context context){
				//key, 当前输入的组的key
				//values, 当前输入的组的所有值
				
				//遍历values
				long count = 0;
				for(LongWritable value : values){
					count += value.get();
				}
				
				//输出这一个单词的统计结果
				context.write(key, new LongWritable(count));
			}
		}

		public class WCRunner {
			public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException{
				Job job = Job.getInstance(new Configuration());
				
				job.setJarByClass(WCRunner.class);		//设置整个job所用的那些类在哪个jar包

				job.setMapperClass(WCMapper.class);		//设置job的mapper类
				job.setReducerClass(WCReducer.class);	//设置job的reducer类
				
				job.setOutputKeyClass(Text.class);		//设置reducer和mapper输出的key-value类
				job.setOutputValueClass(LongWritable.class);	
				
				job.setMapOutputKeyClass(Text.class);	//设置mapper输入的key-value类
				job.setMapOutputValueClass(LongWritable.class);
			
				FileInputFormat.setInputPaths(job, new Path("/wc/srcdata/"));	//数据源位置
				FileOutputFormat.setOutputPath(job, new Path("/wc/output/"));	//结果位置
				
				job.waitForCompletion(true);			//提交job，true是指处理显示中间结果
			} 
		}

	2.运行方式
		1).服务器运行:
			使用工程使用对应的jar包，在服务器上运行指令:
			hadoop jar a.jar b
			其中a.jar是生成的jar包名，由于jar包中有很多类，每个类都可能有main方法，
			因此需要用b来指定运行哪个类。
			这里用的hadoop jar, 因此，里面所指定的路径是hadoop所在目录。
			hadoop jar本质上就是提交jar到集群中区。
		2).本地运行：
			主要是用来检验是否有逻辑错误。
			就直接设置本地文件夹，然后运行。
			相当于是用java jar ... 因此指定的路径是本地路径。
			注意，本地运行，根本没有在集群中分布任务！！
			本地运行时，文件可以从hdfs中去获取，主要是通过用Path去指定，内部会打开FileSystem。
			很明显，打开的FileSystem将会是dfs，然后得到对应的流。
			也就是说，文件从hdfs中获取，然后在本地运行，记录结果。
三、yarn和mapreduce框架
	yarn的管理页面：http://weekend110:8088
	1.节点
		Resource Manager : yarn框架的主节点
		Node Manager	 : yarn框架的程序运行节点
	2.job的资源分配与mapreduce步骤
	------------------------------------------------------------------------------------------
   /|\	1.执行job.waitforcompletion(),启动RunJar进程
	|	2.RunJar进程将向Resource Manager申请1个job
	|	3.Resource Manager返回job相关资源提交的路径和为本job产生的jobID
	|		路径:staging-dir
	资	4.RunJar将资源提交到HDFS的/tmp/xx/xx./yarn-staging/jobID/, 这个资源主要是jar包
	源	5.RunJar汇报提交结果给Resource Manager.
	分	6.rm将本次job加入任务队列（包括job的相关信息）
	配	7.nm周期性俩接rm，并领取任务。
	|	8.nm领取任务后，初始化任务运行环境。一个运行环境就是一个容器，代码就在容器里运行。
	|		因为一个nm里可能有多个job，每个job都有一个容器。
	|		资源容器会去hdfs拿取jar包等等。
   \|/	9.rm随机启动一个nm的容器中的MRAppMaster进程，MRAppMaster管理了整个MapReduce计算。
	------------------------------------------------------------------------------------------
   /|\	10.MRAppMaster向rm注册，MRAppMaster会获得
	|	11.MRAppMaster去启动map Task（MapTask在其他nm上），并对他们进行监视，若有挂掉的，另寻nm启动map
	计	12.MRAppMaster启动Reducer Task
	算	13.MRAppMaster负责了mapper和reducer中数据的传输。
	|	13.每一个Task执行完，MRApper会告诉rm，rm将回收到task已经结算的nm的资源。1
   \|/	14.job完成后，MRAppMaster注销掉自己。
	------------------------------------------------------------------------------------------
	3.RunJar
		RunJar若持有本地客户端，那么就是提交到本地运行。
		RunJar若持有远程代理，那么就是提交到hadoop运行。
		通过配置文件，来进行区别。
			hadoop jar ...	用的hadoop里面的配置项目
			java jar ...	不是用的hadoop里面的配置项
			这个配置项，主要是mapred-site.xml中的mapreduce.framework.name=yarn.
		若希望在本地提交jar包到hadoop集群运行，需要配置的有:
			conf.set("mapreduce.framework.name", "yarn");
			conf.set("yarn.resourcemanager.hostname", "weekend110");	//指定yarn管理机，就跟hdfs里面指定namenode一样
			conf.set("yarn.nodemanager.aux-services", "mapreduce_shuffle");
			conf.set("mapreduce.job.jar", "x.jar");		当然，要生成这个jar包，位置要对哦
		可以出来，实际传到rm的是jar包，方便提取相关的类。
	4.几种运行方式:
		1).在客户端的eclipse里面直接运行main方法，就会将job提交给本地执行器localjobrunner执行
			-----输入输出实际可以在本地路径
			-----输入输出实际也可以在hdfs中
		2).在客户端的eclipse加入mapred-site.xml, yarn-site.xml,将工程打包，并在conf中指定jar包.
			-----输入输出实际可以在本地路径
			-----输入输出实际也可以在hdfs中
		2).在集群中,将工程打包成jar包，上传到服务器，然后用hadoop命令提交
			注意，该服务器需要配置好hadoop，包括rm，mapreduce框架等配置。
	5.yarn的通用性
		由于yarn只管理资源分配，避开了具体的计算模型，因此它不仅适用于mapreduce计算模型。
		storm，spark这类计算模型，也能在yarn框架下使用。但是这些计算框架都需要遵循yarn的协议。
		比如，都需要AppMaster，用于管理这个计算框架本身。
代码分析:
	WaitForCompletion(bollean verbose)
		->submit();
			->ensureState(JobState.DEFINE);		检查状态是否正常
			->connect();						通过cluster封装了动态代理，这个代理可能是远程rm，也可能是本地
				->cluster = ugi.doAs( new PrivilegeExceptionAction<Cluster>
										{
											public Cluster run(){
												return Cluster(getConfiguration());
											}
										});	//doAs的本质就是执行run
			上面创建客户端，下面才是正事提交
			->final JobSubmitter submitter = getJobSubmitter(cluster.getFileSystem(), cluster.getClient());
			->return submitter.submitJobInteral(Job.this, cluster);
		->monitorAndPrintJob();		verbose为true的时候，才执行，用于打印状态
		->return isSuccessful();
			->checkSpecs(job);		检查job的输出，如输出路径已经存在，就抛异常
			->Configuration conf = job.getConfiguration();
			->Path JobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);	拿stageID
			->JobID jobId = submitClient.getNewJobID();
			->job.setJobID(jobID);
			->Path submitJobDir = new Path(jobStagingArea, jobID.toString());	提交资源的真实路径, 资源包括jar包
			->copyAndConfigureFiles(job, submitJobDir);	拷贝资源到路径里去！！！！！
				->Configuration conf = job.getConfiguration();
				->short replication = (short)conf.getInt(Job.SUBMIT_REPLICATION, 10);	mapreduce提交jar包 配置文件的副本数，默认十份。利用hdfs来进行写。
				->copyAndConfigureFiles(job, jobSubmision, short replication);
					->String files = conf.get("tmpfiles");
					->String libjars = conf.get("tmpjars");
					->String archives = conf.get("tmparchives");
					->String jobJar = job.getJar();			//拿到资源jar包名字
					->submitJobDir = new Path(...);			//拿到stage path
					->FileSystem.mkdir(jtFs, submitJobDir, mapredSysPerms);	创建路径
					->Path JobJarPath = new Path(jobJar);	如果jar在集群内存在，就不用从本地拷贝到集群中去
					->copyJar(jobJarPath, JobSubmissionFiles.getJobJar(submitJobDir), replication);	将jar传上去
		
		对于cluster的初始化:
		new Cluster(conf)
			->this(null, conf);
				->this.conf = conf;
				->this.ugi = UserGroupInformation.getCurrentUser();		//ugi == UserGroupInformation,是用户和组的信息，比如hadoop用户
				->initialize(jobTrackAddr, conf);	这里面对其中client成员进行了赋值。
					->provider = new LocalClientProtocolProvider();		首先是创建本地代理
					->clientProtocol = provider.create(conf);
						->framework = conf.get(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME)
						->if(!MRConfig.LOCAL_FRAMEWORK_NAME.equals(framework))->return null
					---------------本地代理执行成功，不会执行下面-----------------
					->provider = new YarnClientProtocolProvider();		然后创建yarn远程代理
					->clientProtocol = provider.create(conf);			根据配置，发现是远程代理，于是生成动态代理
					
		cluster重要成员:
			->ClientProtocol client
			->Path stagingAreaDir	在和rm通信的时候，才会拿到staginAreaDir